{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db28ae93-6077-458a-9a7c-0ea0ab8f1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def is_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell in ('ZMQInteractiveShell',)  # Jupyter Notebook or JupyterLab\n",
    "    except NameError:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_02-deploying-multi-stage-recsys-with-merlin-systems/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Deploying a Multi-Stage RecSys into Production with Merlin Systems and Triton Inference Server\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "At this point, when you reach out to this notebook, we expect that you have already executed the first notebook `01-Building-Recommender-Systems-with-Merlin.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![tritonensemble](../images/triton_ensemble.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "Merlin Systems library have the set of operators to be able to serve multi-stage recommender systems built with Tensorflow on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently. Below, we will go through these operators and demonstrate their usage in serving a multi-stage system on Triton."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a27e18d7-b3e4-481c-b69e-23193b212c56",
   "metadata": {},
   "source": [
    "At this step, we assume you already installed feast and faiss-gpu (or -cpu) libraries when running the first notebook `01-Building-Recommender-Systems-with-Merlin.ipynb`. \n",
    "\n",
    "In case you need to install them again, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e658308d-e690-46bb-9b79-cfe22190f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feast==0.31 in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (8.1.7)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.4.6)\n",
      "Requirement already satisfied: dill~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.3.9)\n",
      "Requirement already satisfied: fastavro<2,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.11.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.47.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.73.0)\n",
      "Requirement already satisfied: grpcio-reflection<2,>=1.47.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.62.3)\n",
      "Requirement already satisfied: Jinja2<4,>=2 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (3.1.3)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (4.20.0)\n",
      "Requirement already satisfied: mmh3 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (5.1.0)\n",
      "Requirement already satisfied: numpy<3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.22.4)\n",
      "Requirement already satisfied: pandas<2,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.5.2)\n",
      "Requirement already satisfied: pandavro~=1.5.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.5.2)\n",
      "Requirement already satisfied: protobuf<5,>3.20 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (4.25.8)\n",
      "Requirement already satisfied: proto-plus<2,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.26.1)\n",
      "Requirement already satisfied: pyarrow<12,>=4 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (10.0.1.dev0+ga6eabc2b.d20230428)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.10.22)\n",
      "Requirement already satisfied: pygments<3,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2.17.2)\n",
      "Requirement already satisfied: PyYAML<7,>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>1 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[mypy]<2,>1->feast==0.31) (1.4.54)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.9.0)\n",
      "Requirement already satisfied: tenacity<9,>=7 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (8.5.0)\n",
      "Requirement already satisfied: toml<1,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.10.2)\n",
      "Requirement already satisfied: tqdm<5,>=4 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (4.66.1)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2.13.3)\n",
      "Requirement already satisfied: fastapi<1,>=0.68.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.115.13)\n",
      "Requirement already satisfied: uvicorn<1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]<1,>=0.14.0->feast==0.31) (0.34.3)\n",
      "Requirement already satisfied: dask>=2021.1.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2023.1.1)\n",
      "Requirement already satisfied: bowler in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.9.0)\n",
      "Requirement already satisfied: httpx>=0.23.3 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (0.28.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (2022.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (23.2)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (1.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (0.12.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.68.0->feast==0.31) (0.46.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.68.0->feast==0.31) (4.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (4.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (3.6)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.3->feast==0.31) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2->feast==0.31) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.4.3->feast==0.31) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.4.3->feast==0.31) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/lib/python3/dist-packages (from pandavro~=1.5.0->feast==0.31) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>1->SQLAlchemy[mypy]<2,>1->feast==0.31) (3.0.3)\n",
      "Requirement already satisfied: sqlalchemy2-stubs in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[mypy]<2,>1->feast==0.31) (0.0.2a38)\n",
      "Requirement already satisfied: mypy>=0.910 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[mypy]<2,>1->feast==0.31) (1.16.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]<1,>=0.14.0->feast==0.31) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]<1,>=0.14.0->feast==0.31) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]<1,>=0.14.0->feast==0.31) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]<1,>=0.14.0->feast==0.31) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]<1,>=0.14.0->feast==0.31) (15.0.1)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from bowler->feast==0.31) (23.2.0)\n",
      "Requirement already satisfied: fissix in /usr/local/lib/python3.10/dist-packages (from bowler->feast==0.31) (24.4.24)\n",
      "Requirement already satisfied: moreorless>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from bowler->feast==0.31) (0.5.0)\n",
      "Requirement already satisfied: volatile in /usr/local/lib/python3.10/dist-packages (from bowler->feast==0.31) (2.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast==0.31) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast==0.31) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast==0.31) (0.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->feast==0.31) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->feast==0.31) (2.0.7)\n",
      "Requirement already satisfied: mypy_extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast==0.31) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast==0.31) (0.12.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast==0.31) (2.0.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask>=2021.1.0->feast==0.31) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.3->feast==0.31) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.3->feast==0.31) (1.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from fissix->bowler->feast==0.31) (1.4.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"feast==0.31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce807cb3-c1a0-4bab-b2e9-b863bbb4f519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-cpu==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (1.73.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (3.10.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (0.4.23)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (16.0.6)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (4.25.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow-cpu==2.12.0) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu==2.12.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu==2.12.0) (0.35.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow-cpu==2.12.0) (0.37.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow-cpu==2.12.0) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow-cpu==2.12.0) (1.11.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found existing installation: cudf 23.4.0\n",
      "Can't uninstall 'cudf'. No files were found to uninstall.\n",
      "Found existing installation: horovod 0.28.0+nv23.6\n",
      "Can't uninstall 'horovod'. No files were found to uninstall.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# CHOOSE AN OPTION - GPU or CPU:\n",
    "\n",
    "# for running this example on GPU, uncomment the following lines\n",
    "# !pip install faiss-gpu\n",
    "# import os\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "# for running this example on CPU, uncomment the following lines\n",
    "# ATTENTION: The installation of \"tensorflow-cpu==2.12.0\" will overwrite the installation of tensorflow, which ruin the abillity for Triton Inference Server to serve the models.\n",
    "#            So to serve the models, you would need to use a new container of `merlin-tensorflow` that has the original \"tensorflow\" package.\n",
    "# !pip install \"tensorflow-cpu==2.12.0\" faiss-cpu\n",
    "# !pip uninstall --yes cudf horovod\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "2025-06-25 08:07:46.221994: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/nvtabular/loader/__init__.py:19: DeprecationWarning: The `nvtabular.loader` module has moved to a new repository, at https://github.com/NVIDIA-Merlin/dataloader .  Support for importing from `nvtabular.loader` is deprecated, and will be removed in a future version. Please update your imports to refer to `merlinloader`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feast\n",
    "import seedir as sd\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55ead20e-c573-462e-9aa2-c3494bf0129f",
   "metadata": {},
   "source": [
    "### Register our features on feature store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2ac115e-4794-4a69-a962-8481f6e86df3",
   "metadata": {},
   "source": [
    "The Feast feature registry is a central catalog of all the feature definitions and their related metadata(read more [here](https://docs.feast.dev/getting-started/architecture-and-components/registry)). We have defined our user and item features definitions in the `user_features.py` and  `item_features.py` files. With FeatureView() users can register data sources in their organizations into Feast, and then use those data sources for both training and online inference. In the `user_features.py` and `item_features.py` files, we are telling Feast where to find user and item features.\n",
    "\n",
    "Before we move on to the next steps, we need to perform `feast apply`command as directed below.  With that, we register our features, we can apply the changes to create our feature registry and store all entity and feature view definitions in a local SQLite online store called `online_store.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c02d67-df45-4869-8262-647cba77efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/Merlin/examples/Building-and-deploying-multi-stage-RecSys/\")\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "\n",
    "\n",
    "# define feature repo path\n",
    "feast_repo_path = os.path.join(BASE_DIR, \"feast_repo/feature_repo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3508e3-b0bb-484d-a8f9-66f9b44510ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Merlin/examples/Building-and-deploying-multi-stage-RecSys/feast_repo/feature_repo/\n"
     ]
    }
   ],
   "source": [
    "print(feast_repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5fa545b-a979-4216-b176-ffd70d66e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Merlin/examples/Building-and-deploying-multi-stage-RecSys/feast_repo/feature_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entity \u001b[1m\u001b[32muser_id\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mitem_id\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mitem_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32muser_features\u001b[0m\n",
      "\n",
      "Created sqlite table \u001b[1m\u001b[32mfeast_repo_item_features\u001b[0m\n",
      "Created sqlite table \u001b[1m\u001b[32mfeast_repo_user_features\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd $feast_repo_path\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e66d0-513e-4267-b6bd-6f35269aebbb",
   "metadata": {},
   "source": [
    "### Optionally, view the feast store via the Web Dashboard\n",
    "\n",
    "```\n",
    "cd /Merlin/examples/Building-and-deploying-multi-stage-RecSys/feast_repo/feature_repo\n",
    "feast ui --port 8889\n",
    "```\n",
    "\n",
    "Then view it at http://localhost:8889"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c641fcd2-bd11-4569-80d4-2ae5e01a5cad",
   "metadata": {},
   "source": [
    "### Loading features from offline store into an online store \n",
    "\n",
    "After we execute `apply` and registered our features and created our online local store, now we need to perform [materialization](https://docs.feast.dev/how-to-guides/running-feast-in-production) operation. This is done to keep our online store up to date and get it ready for prediction. For that we need to run a job that loads feature data from our feature view sources into our online store. As we add new features to our offline stores, we can continuously materialize them to keep our online store up to date by finding the latest feature values for each user. \n",
    "\n",
    "When you run the `feast materialize ..` command below, you will see a message <i>Materializing 2 feature views from 1995-01-01 01:01:01+00:00 to 2025-01-01 01:01:01+00:00 into the sqlite online store </i>  will be printed out.\n",
    "\n",
    "Note that materialization step takes some time.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52dacbbc-bdb6-4f7a-b202-3802050f0362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m1995-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m:\n",
      "0it [00:00, ?it/s]\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m:\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fcc26e6-f6f3-4e44-bf3c-3b8e66dc9fd6",
   "metadata": {},
   "source": [
    "Now, let's check our feature_repo structure again after we ran `apply` and `materialize` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9caba4e3-e6e0-4e2f-b51d-cd3456fd4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feast_repo/\n",
      "├─README.md\n",
      "├─__init__.py\n",
      "└─feature_repo/\n",
      "  ├─__init__.py\n",
      "  ├─data/\n",
      "  │ ├─item_features.parquet\n",
      "  │ ├─online_store.db\n",
      "  │ ├─registry.db\n",
      "  │ └─user_features.parquet\n",
      "  ├─feature_store.yaml\n",
      "  ├─item_features.py\n",
      "  ├─test_workflow.py\n",
      "  └─user_features.py\n"
     ]
    }
   ],
   "source": [
    "# set up the base dir to for feature store\n",
    "sd.seedir(os.path.join(BASE_DIR, 'feast_repo'), style='lines', itemlimit=10, depthlimit=5, exclude_folders=['.ipynb_checkpoints', '__pycache__'], sort=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e768637c-0a4d-404b-8b58-7182fef0ab0e",
   "metadata": {},
   "source": [
    "### Set up Faiss index, create feature store client and objects for the Triton ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efada1e1-2556-4a26-b0ba-9cb96b3b151f",
   "metadata": {},
   "source": [
    "Create a folder for faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b7adc1-623b-41df-b1f9-dd4086a15bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR, 'faiss_index')):\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'faiss_index'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking model, retrieval model, and faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = os.path.join(BASE_DIR, 'faiss_index', \"index.faiss\")\n",
    "retrieval_model_path = os.path.join(BASE_DIR, \"query_tower/\")\n",
    "ranking_model_path = os.path.join(BASE_DIR, \"dlrm/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b996019-bd2a-44e0-b004-4f412b300d63",
   "metadata": {},
   "source": [
    "`QueryFaiss` operator creates an interface between a FAISS Approximate Nearest Neighbors (ANN) Index and Triton Inference Server. For a given input query vector, we do an ANN search query to find the ids of top-k nearby nodes in the index.\n",
    "\n",
    "`setup_faiss` is  a utility function that will create a Faiss index from an embedding vector with using L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6cc5bf-d07c-4963-a748-6e2b4827ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 431 points to 32 centroids: please provide at least 1248 training points\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss \n",
    "\n",
    "item_embeddings = pd.read_parquet(os.path.join(BASE_DIR, \"item_embeddings.parquet\"))\n",
    "setup_faiss(item_embeddings, faiss_index_path, embedding_column=\"output_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46697177-512a-473e-8cca-9fe51d3daa03",
   "metadata": {},
   "source": [
    "Create feature store client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc00e04-c70c-4882-9952-66f4dbb97bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = feast.FeatureStore(feast_repo_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c45df06-0cbe-4b52-ac1f-786e763895d7",
   "metadata": {},
   "source": [
    "Fetch user features with `QueryFeast` operator from the feature store. `QueryFeast` operator is responsible for ensuring that our feast feature store can communicate correctly with tritonserver for the ensemble feast feature look ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3decbe7b-03e3-4978-baac-03f6a0b078c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2025-06-25 08:07:53+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m from \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-06-25 08:07:53+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 459/459 [00:00<00:00, 1598.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "user_attributes = [\"user_id\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11299b6-20d4-4687-bb0e-b855a9bcb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular import Workflow\n",
    "\n",
    "nvt_workflow = Workflow.load(os.path.join(DATA_FOLDER, 'processed_nvt/workflow'))\n",
    "user_subgraph = nvt_workflow.get_subworkflow(\"user\")\n",
    "user_features = user_attributes >> TransformWorkflow(user_subgraph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27e25be7-3ff0-49c2-a3fc-03ec4d615e77",
   "metadata": {},
   "source": [
    "Retrieve top-K candidate items using `retrieval model` that are relevant for a given user. We use `PredictTensorflow()` operator that takes a tensorflow model and packages it correctly for TIS to run with the tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21139caa-3a51-42e6-b006-21a92c95f1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevent TF to claim all GPU memory\n",
    "from merlin.dataloader.tf_utils import configure_tensorflow\n",
    "\n",
    "\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c2d9b1-51dc-4549-977d-d7941ee6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnsbl0mla/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnsbl0mla/assets\n"
     ]
    }
   ],
   "source": [
    "topk_retrieval = int(\n",
    "    os.environ.get(\"topk_retrieval\", \"100\")\n",
    ")\n",
    "retrieval = (\n",
    "    user_features\n",
    "    >> PredictTensorflow(retrieval_model_path)\n",
    "    >> QueryFaiss(faiss_index_path, topk=topk_retrieval)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce4429c-1fe1-4304-bcdf-badebe3b5485",
   "metadata": {},
   "source": [
    "Fetch item features for the candidate items that are retrieved from the retrieval step above from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b270f663-0ae1-4356-acd4-5f8c986abf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2025-06-25 08:07:58+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m from \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-06-25 08:07:58+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 431/431 [00:00<00:00, 2931.27it/s]\n"
     ]
    }
   ],
   "source": [
    "item_attributes = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d0a4531-665c-48a1-98a9-216c955449b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subgraph = nvt_workflow.get_subworkflow(\"item\")\n",
    "item_features = item_attributes >> TransformWorkflow(item_subgraph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "304a4d09-db05-4666-b520-75dbbbc7ab17",
   "metadata": {},
   "source": [
    "Merge the user features and items features to create the all set of combined features that were used in model training using `UnrollFeatures` operator which takes a target column and joins the \"unroll\" columns to the target. This helps when broadcasting a series of user features to a set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb0ef434-03a5-4a36-afb9-e19a43243c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fb0ce66-6b6c-43be-885e-a5435c3bbd9e",
   "metadata": {},
   "source": [
    "Rank the combined features using the trained ranking model, which is a DLRM model for this example. We feed the path of the ranking model to `PredictTensorflow()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce31723e-af4d-4827-bb60-3a9fafcd9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 98). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmple_uovry/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmple_uovry/assets\n"
     ]
    }
   ],
   "source": [
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f86fa47-de61-4007-ab55-9076e12ce963",
   "metadata": {},
   "source": [
    "For the ordering we use `SoftmaxSampling()` operator. This operator sorts all inputs in descending order given the input ids and prediction introducing some randomization into the ordering by sampling items from the softmax of the predicted relevance scores, and finally returns top-k ordered items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f65598b-e3e7-4238-a73e-19d00c3deb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=10\n",
    "ordering = combined_features[\"item_id\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"click/binary_classification_task\"], topk=top_k, temperature=0.00000001\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4e2e389-d884-44a1-8e32-4916a0eb43cf",
   "metadata": {},
   "source": [
    "### Export Graph as Ensemble\n",
    "The last step is to create the ensemble artifacts that TIS can consume. To make these artifacts import the Ensemble class. This class  represents an entire ensemble consisting of multiple models that run sequentially in TIS initiated by an inference request. It is responsible with interpreting the graph and exporting the correct files for TIS.\n",
    "\n",
    "When we create an Ensemble object we feed the graph and a schema representing the starting input of the graph.  After we create the ensemble object, we export the graph, supplying an export path for the `ensemble.export()` function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50bc2e4f-5e58-4ad4-8ae5-d79ad286978f",
   "metadata": {},
   "source": [
    "Create the folder to export the models and config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b28c452f-543c-45a4-9995-130ca6919669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR, 'poc_ensemble')):\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'poc_ensemble'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a061bd82-e553-4823-8d14-3ae88a458c14",
   "metadata": {},
   "source": [
    "Create a request schema that we are going to use when sending a request to Triton Inference Server (TIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c8b7b94-5559-4587-a272-4d9de2d53dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d091f82-58c7-45b9-9ceb-c12511b47d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_id</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'user_id', 'tags': set(), 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if is_notebook():\n",
    "    display(request_schema)\n",
    "else:\n",
    "    print(request_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c64d686-aed5-42f8-b517-482b4237c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ordered_ids', 'ordered_scores']\n"
     ]
    }
   ],
   "source": [
    "# define the path where all the models and config files exported to\n",
    "export_path = os.path.join(BASE_DIR, 'poc_ensemble')\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)\n",
    "\n",
    "# return the output column name\n",
    "outputs = ensemble.graph.output_schema.column_names\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2bb6f6d-2788-4edb-9e7d-9e6a702b4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write the \"request_schema\" and \"outputs\" to files:\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"request_schema.pkl\", \"wb\") as f:\n",
    "    pickle.dump(request_schema, f)\n",
    "\n",
    "with open(\"outputs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(outputs, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "276eedd8-5dc0-4ad0-8725-c8da60fea693",
   "metadata": {},
   "source": [
    "Let's check our export_path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89182219-40a6-458c-af0e-7a8e83f364aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poc_ensemble/\n",
      "├─0_transformworkflowtriton/\n",
      "│ ├─1/\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ │   ├─categories/\n",
      "│ │   │ ├─unique.user_age.parquet\n",
      "│ │   │ ├─unique.user_brands.parquet\n",
      "│ │   │ ├─unique.user_categories.parquet\n",
      "│ │   │ ├─unique.user_consumption_2.parquet\n",
      "│ │   │ ├─unique.user_gender.parquet\n",
      "│ │   │ ├─unique.user_geography.parquet\n",
      "│ │   │ ├─unique.user_group.parquet\n",
      "│ │   │ ├─unique.user_id.parquet\n",
      "│ │   │ ├─unique.user_intentions.parquet\n",
      "│ │   │ └─unique.user_is_occupied.parquet\n",
      "│ │   ├─metadata.json\n",
      "│ │   └─workflow.pkl\n",
      "│ └─config.pbtxt\n",
      "├─1_predicttensorflowtriton/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─assets/\n",
      "│ │   ├─fingerprint.pb\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2_transformworkflowtriton/\n",
      "│ ├─1/\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ │   ├─categories/\n",
      "│ │   │ ├─unique.item_brand.parquet\n",
      "│ │   │ ├─unique.item_category.parquet\n",
      "│ │   │ ├─unique.item_id.parquet\n",
      "│ │   │ └─unique.item_shop.parquet\n",
      "│ │   ├─metadata.json\n",
      "│ │   └─workflow.pkl\n",
      "│ └─config.pbtxt\n",
      "├─3_predicttensorflowtriton/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ ├─input_schema.json\n",
      "│ │   │ └─output_schema.json\n",
      "│ │   ├─assets/\n",
      "│ │   ├─fingerprint.pb\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "└─executor_model/\n",
      "  ├─1/\n",
      "  │ ├─ensemble/\n",
      "  │ │ ├─ensemble.pkl\n",
      "  │ │ ├─index.faiss\n",
      "  │ │ └─metadata.json\n",
      "  │ └─model.py\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "sd.seedir(export_path, style='lines', itemlimit=10, depthlimit=5, exclude_folders=['.ipynb_checkpoints', '__pycache__'], sort=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe7962cc-f26d-4a4a-b5a3-d214e0f37456",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Starting Triton Server"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c07c620-7d6c-4275-87fe-e5b94335bdb9",
   "metadata": {},
   "source": [
    "It is time to deploy all the models as an ensemble model to Triton Inference Serve [TIS](https://github.com/triton-inference-server). After we export the ensemble, we are ready to start the TIS. You can start triton server by using the following command on your terminal:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/ensemble_export_path/ --backend-config=tensorflow,allow-soft-placement=true\n",
    "```\n",
    "\n",
    "For the `--model-repository` argument, specify the same path as the `export_path` that you specified previously in the `ensemble.export` method. This command will launch the server and load all the models to the server. Once all the models are loaded successfully, you should see `READY` status printed out in the terminal for each loaded model.\n",
    "\n",
    "In our case:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/Merlin/examples/Building-and-deploying-multi-stage-RecSys/poc_ensemble --backend-config=tensorflow,allow-soft-placement=true\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0794b1-b9e0-4508-bf6e-cc823ac5c693",
   "metadata": {},
   "source": [
    "Once our models are successfully loaded to the TIS, we can now easily send a request to TIS and get a response for our query with `send_triton_request` utility function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af9efbde-4dac-42f1-9ace-096f75bac2b5",
   "metadata": {},
   "source": [
    "Let's send a request to TIS for a given `user_id_raw` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4419e7f4-ec6b-44a2-9c81-318e5746c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feast==0.31\n",
      "  Downloading feast-0.31.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (8.1.7)\n",
      "Collecting colorama<1,>=0.3.9 (from feast==0.31)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting dill~=0.3.0 (from feast==0.31)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastavro<2,>=1.1.0 (from feast==0.31)\n",
      "  Downloading fastavro-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: grpcio<2,>=1.47.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.60.0)\n",
      "Collecting grpcio-reflection<2,>=1.47.0 (from feast==0.31)\n",
      "  Downloading grpcio_reflection-1.73.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (3.1.3)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (4.20.0)\n",
      "Collecting mmh3 (from feast==0.31)\n",
      "  Downloading mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.22.4)\n",
      "Requirement already satisfied: pandas<2,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (1.5.2)\n",
      "Collecting pandavro~=1.5.0 (from feast==0.31)\n",
      "  Downloading pandavro-1.5.2.tar.gz (3.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf<5,>3.20 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (3.20.3)\n",
      "Collecting proto-plus<2,>=1.20.0 (from feast==0.31)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pyarrow<12,>=4 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (10.0.1.dev0+ga6eabc2b.d20230428)\n",
      "Collecting pydantic<2,>=1 (from feast==0.31)\n",
      "  Downloading pydantic-1.10.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2.17.2)\n",
      "Requirement already satisfied: PyYAML<7,>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2.31.0)\n",
      "Collecting SQLAlchemy<2,>1 (from SQLAlchemy[mypy]<2,>1->feast==0.31)\n",
      "  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting tabulate<1,>=0.8.0 (from feast==0.31)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tenacity<9,>=7 (from feast==0.31)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<1,>=0.10.0 (from feast==0.31)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (4.66.1)\n",
      "Collecting typeguard==2.13.3 (from feast==0.31)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting fastapi<1,>=0.68.0 (from feast==0.31)\n",
      "  Downloading fastapi-0.115.13-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn<1,>=0.14.0 (from uvicorn[standard]<1,>=0.14.0->feast==0.31)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: dask>=2021.1.0 in /usr/local/lib/python3.10/dist-packages (from feast==0.31) (2023.1.1)\n",
      "Collecting bowler (from feast==0.31)\n",
      "  Downloading bowler-0.9.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx>=0.23.3 (from feast==0.31)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (2022.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (23.2)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (1.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.1.0->feast==0.31) (0.12.0)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1,>=0.68.0->feast==0.31)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.68.0->feast==0.31) (4.9.0)\n",
      "INFO: pip is looking at multiple versions of grpcio-reflection to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-reflection<2,>=1.47.0 (from feast==0.31)\n",
      "  Downloading grpcio_reflection-1.73.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading grpcio_reflection-1.72.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading grpcio_reflection-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Downloading grpcio_reflection-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-reflection to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_reflection-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_reflection-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_reflection-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting protobuf<5,>3.20 (from feast==0.31)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting grpcio<2,>=1.47.0 (from feast==0.31)\n",
      "  Downloading grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (4.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.3->feast==0.31)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.3->feast==0.31) (3.6)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.23.3->feast==0.31)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2->feast==0.31) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.4.3->feast==0.31) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.4.3->feast==0.31) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/lib/python3/dist-packages (from pandavro~=1.5.0->feast==0.31) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>1->SQLAlchemy[mypy]<2,>1->feast==0.31) (3.0.3)\n",
      "Collecting sqlalchemy2-stubs (from SQLAlchemy[mypy]<2,>1->feast==0.31)\n",
      "  Downloading sqlalchemy2_stubs-0.0.2a38-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting mypy>=0.910 (from SQLAlchemy[mypy]<2,>1->feast==0.31)\n",
      "  Downloading mypy-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]<1,>=0.14.0->feast==0.31)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]<1,>=0.14.0->feast==0.31)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]<1,>=0.14.0->feast==0.31)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]<1,>=0.14.0->feast==0.31)\n",
      "  Downloading watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]<1,>=0.14.0->feast==0.31)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from bowler->feast==0.31) (23.2.0)\n",
      "Collecting fissix (from bowler->feast==0.31)\n",
      "  Downloading fissix-24.4.24-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting moreorless>=0.2.0 (from bowler->feast==0.31)\n",
      "  Downloading moreorless-0.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting volatile (from bowler->feast==0.31)\n",
      "  Downloading volatile-2.1.0.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast==0.31) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast==0.31) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast==0.31) (0.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->feast==0.31) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->feast==0.31) (2.0.7)\n",
      "Requirement already satisfied: mypy_extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast==0.31) (1.0.0)\n",
      "Collecting pathspec>=0.9.0 (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast==0.31)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast==0.31) (2.0.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask>=2021.1.0->feast==0.31) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.3->feast==0.31) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.3->feast==0.31) (1.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from fissix->bowler->feast==0.31) (1.4.4)\n",
      "Downloading feast-0.31.0-py2.py3-none-any.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.13-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_reflection-1.62.3-py3-none-any.whl (22 kB)\n",
      "Downloading grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m301.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m844.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bowler-0.9.0-py3-none-any.whl (36 kB)\n",
      "Downloading mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading moreorless-0.5.0-py3-none-any.whl (14 kB)\n",
      "Downloading mypy-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m439.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fissix-24.4.24-py3-none-any.whl (188 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy2_stubs-0.0.2a38-py3-none-any.whl (191 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.8/191.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: pandavro, volatile\n",
      "  Building wheel for pandavro (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandavro: filename=pandavro-1.5.2-py3-none-any.whl size=2953 sha256=5fa2a3322b557e712036f513fdc42754fe8ae9e05159464dbfa3796bba2f7567\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/68/1c/fbbf5ef5ca4f0c7661345cfaf94a0def2d3de07683e1e8c1cf\n",
      "  Building wheel for volatile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for volatile: filename=volatile-2.1.0-py3-none-any.whl size=3468 sha256=1a2f205328ebde645e986c5f2a207fd6867af3f808e5758fa5fa1ba02e63577b\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/e6/63/e33c0d197f811ac41710d479f79648b4a0fbc3db868cdde17c\n",
      "Successfully built pandavro volatile\n",
      "Installing collected packages: volatile, websockets, uvloop, typeguard, toml, tenacity, tabulate, sqlalchemy2-stubs, SQLAlchemy, python-dotenv, pydantic, protobuf, pathspec, moreorless, mmh3, httptools, h11, grpcio, fissix, fastavro, dill, colorama, watchfiles, uvicorn, starlette, proto-plus, mypy, httpcore, grpcio-reflection, bowler, pandavro, httpx, fastapi, feast\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.60.0\n",
      "    Uninstalling grpcio-1.60.0:\n",
      "      Successfully uninstalled grpcio-1.60.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.4.0 requires cupy-cuda12x, which is not installed.\n",
      "dask-cudf 23.4.0 requires cupy-cuda12x, which is not installed.\n",
      "transformers4rec 23.12.0 requires torchmetrics>=0.10.0, which is not installed.\n",
      "cudf 23.4.0 requires cuda-python>=12, but you have cuda-python 11.8.3 which is incompatible.\n",
      "cudf 23.4.0 requires numba<0.57,>=0.56.4, but you have numba 0.58.1 which is incompatible.\n",
      "cudf 23.4.0 requires protobuf==3.20.3, but you have protobuf 4.25.8 which is incompatible.\n",
      "dask-cudf 23.4.0 requires dask==2023.3.2, but you have dask 2023.1.1 which is incompatible.\n",
      "dask-cudf 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2023.1.1 which is incompatible.\n",
      "merlin-core 23.8.0 requires fsspec>=2022.7.1, but you have fsspec 2022.5.0 which is incompatible.\n",
      "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.8 which is incompatible.\n",
      "tf2onnx 1.16.0 requires protobuf~=3.20.2, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SQLAlchemy-1.4.54 bowler-0.9.0 colorama-0.4.6 dill-0.3.9 fastapi-0.115.13 fastavro-1.11.1 feast-0.31.0 fissix-24.4.24 grpcio-1.73.1 grpcio-reflection-1.62.3 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 mmh3-5.1.0 moreorless-0.5.0 mypy-1.16.1 pandavro-1.5.2 pathspec-0.12.1 proto-plus-1.26.1 protobuf-4.25.8 pydantic-1.10.22 python-dotenv-1.1.1 sqlalchemy2-stubs-0.0.2a38 starlette-0.46.2 tabulate-0.9.0 tenacity-8.5.0 toml-0.10.2 typeguard-2.13.3 uvicorn-0.34.3 uvloop-0.21.0 volatile-2.1.0 watchfiles-1.1.0 websockets-15.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"feast==0.31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2666eb64-5bce-40ad-bf77-01a305cb68e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast is an open source project that collects anonymized error reporting and usage statistics. To opt out or learn more see https://docs.feast.dev/reference/usage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "2025-06-26 06:21:17.841227: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/nvtabular/loader/__init__.py:19: DeprecationWarning: The `nvtabular.loader` module has moved to a new repository, at https://github.com/NVIDIA-Merlin/dataloader .  Support for importing from `nvtabular.loader` is deprecated, and will be removed in a future version. Please update your imports to refer to `merlinloader`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feast\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332b575f-730b-4ef4-82b3-ad0f51965bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/Merlin/examples/Building-and-deploying-multi-stage-RecSys/\")\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "\n",
    "\n",
    "# define feature repo path\n",
    "feast_repo_path = os.path.join(BASE_DIR, \"feast_repo/feature_repo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a2a0cd-c967-4775-bbf5-fbdccaf98b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the \"request_schema\" and \"outputs\" from files:\n",
    "import pickle\n",
    "\n",
    "with open(f\"{feast_repo_path}/request_schema.pkl\", \"rb\") as f:\n",
    "    request_schema = pickle.load(f)\n",
    "\n",
    "with open(f\"{feast_repo_path}/outputs.pkl\", \"rb\") as f:\n",
    "    outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08a8975-9c32-467b-99ec-df66319f854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ request_schema ------\n",
      "[{'name': 'user_id', 'tags': set(), 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}]\n",
      "------ request ------\n",
      "   user_id\n",
      "0       10\n",
      "------ outputs ------\n",
      "['ordered_ids', 'ordered_scores']\n"
     ]
    }
   ],
   "source": [
    "# read in data for request\n",
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "# create a request to be sent to TIS\n",
    "request = make_df({\"user_id\": [10]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)\n",
    "\n",
    "# Debug log\n",
    "print(\"------ request_schema ------\")\n",
    "print(request_schema)\n",
    "print(\"------ request ------\")\n",
    "print(request)\n",
    "print(\"------ outputs ------\")\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28e9e27f-6658-4302-b142-08b05215e48f",
   "metadata": {},
   "source": [
    "Let's return raw item ids from TIS as top-k recommended items per given request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ec62f2-5935-45c6-8058-e1cdade6f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ordered_ids': array([[ 39,  48, 198, 155, 136,  26, 221, 202, 323, 334]], dtype=int32), 'ordered_scores': array([[0.50128824, 0.5016367 , 0.5016628 , 0.50175   , 0.5021679 ,\n",
      "        0.5010164 , 0.5012386 , 0.5010482 , 0.50221455, 0.5017442 ]],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    response = send_triton_request(\n",
    "        request_schema,\n",
    "        request,\n",
    "        outputs,\n",
    "        endpoint=\"merlin-serving:8001\",\n",
    "    )\n",
    "    print(response)\n",
    "except:\n",
    "    input_columns = request.columns\n",
    "    print(\n",
    "        \"Error: The operation failed due to one of the following reasons:\\n\"\n",
    "        \" - Out of memory: The GPU may not have sufficient VRAM. Please check GPU memory allocation using `nvidia-smi`.\\n\"\n",
    "        f\" - Invalid input: The provided inputs may be incorrect. Please verify the input field(s): {input_columns}.\",\n",
    "        file=sys.stderr\n",
    "      )\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "That's it! You finished deploying a multi-stage Recommender Systems on Triton Inference Server using Merlin framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452cb0a-8028-470b-b738-bce494f62ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow-inference:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
